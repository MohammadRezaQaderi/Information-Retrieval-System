{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_phase1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PHASE I"
      ],
      "metadata": {
        "id": "QIUAEpIo4NZm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRgP16Yvkdr-"
      },
      "source": [
        "pip install hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st5Bw-zmx2Qf"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hazm import *\n",
        "import collections \n",
        "import string\n",
        "import operator\n",
        "import datetime\n",
        "import json\n",
        "import math\n",
        "import copy\n",
        "import re"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the file \n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "mZaLcNeLbT99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22M5ZSWfzzKl"
      },
      "source": [
        "# Read file from the Google colud\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1jcbbbPTNnQ3NKrPJJL_oc9vrFvHH4UuR'\n",
        "downloaded = drive.CreateFile({'id': file_id})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWQb8M-G0lNd"
      },
      "source": [
        "downloaded.GetContentFile('/IR1_7k_news.xlsx')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc6yuDCtyi3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d2c1b1-5368-454c-86a3-233ad786946f"
      },
      "source": [
        "# read data\n",
        "# df = pd.read_excel(\"./IR1_7k_news.xlsx\")\n",
        "df = pd.read_excel(\"/IR1_7k_news.xlsx\")\n",
        "urls = df['url']\n",
        "content = df['content']\n",
        "title = df['title']\n",
        "# normalize data by use hazm\n",
        "normalizer = Normalizer()\n",
        "print(content[11])\n",
        "for i in range(len(content)):\n",
        "  content[i] = normalizer.normalize(content[i])\n",
        "copy_content = content\n",
        "print(content[11])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "به گزارش خبرگزاری فارس از **اصفهان،** دیدار تیم‌های ذوب‌آهن و استقلال با 2 گل\n",
            "به سود شاگردان [فرهاد مجیدی](https://www.farsnews.ir/special/فرهاد مجیدی) به\n",
            "پایان رسید.\n",
            "\n",
            "بعد از این مسابقه [رشید مظاهری](https://search.farsnews.ir/?q=رشید\n",
            "مظاهری&o=on) در رختکن ذوب‌آهن حاضر شد و با بازیکنان تیم میزبان خوش وبش کرد.\n",
            "مظاهری بعد از خروج از رختکن به درخواست خبرنگاران برای مصاحبه پاسخ منفی داد و\n",
            "گفت آنقدر مشکل روحی و روانی دارم که اصلاً نمی‌توانم مصاحبه کنم.\n",
            "\n",
            "به گزارش فارس، ظاهراً طی روزهای گذشته مشکلاتی برای دروازه‌بان استقلال در\n",
            "تمرینات این تیم به وجود آمده که وی [به صورت](https://search.farsnews.ir/?q=به\n",
            "صورت&o=on) سربسته مقابل خبرنگاران به این موضوع اشاره کرده است.\n",
            "\n",
            "انتهای پیام/\n",
            "\n",
            "\n",
            "به گزارش خبرگزاری فارس از **اصفهان، ** دیدار تیم‌های ذوب‌آهن و استقلال با ۲ گل\n",
            "به سود شاگردان [فرهاد مجیدی] (https: //www. farsnews. ir/special/فرهاد مجیدی) به\n",
            "پایان رسید. \n",
            "\n",
            "بعد از این مسابقه [رشید مظاهری] (https: //search. farsnews. ir/?q=رشید\n",
            "مظاهری&o=on) در رختکن ذوب‌آهن حاضر شد و با بازیکنان تیم میزبان خوش وبش کرد. \n",
            "مظاهری بعد از خروج از رختکن به درخواست خبرنگاران برای مصاحبه پاسخ منفی داد و\n",
            "گفت آنقدر مشکل روحی و روانی دارم که اصلا نمی‌توانم مصاحبه کنم. \n",
            "\n",
            "به گزارش فارس، ظاهرا طی روزهای گذشته مشکلاتی برای دروازه‌بان استقلال در\n",
            "تمرینات این تیم به وجود آمده که وی [به صورت] (https: //search. farsnews. ir/?q=به\n",
            "صورت&o=on) سربسته مقابل خبرنگاران به این موضوع اشاره کرده است. \n",
            "\n",
            "انتهای پیام/\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN44N6fh0YXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c436c8-19bc-4153-e941-02e3fb8df590"
      },
      "source": [
        "punctuation  = [ '+','=', '-' , '*' , '.' ,'?', '[', ']','(',')','{','}','<','>', '«' , '»' ,':' , \"؟\", \"؛\" , \"،\", \"،\" ]\n",
        "signs = [ '!' , '@' , '#' , '$' , '%' , '^' , '&' , '*', '_', '\\\\' , '/' , '//' , '|' , \"…\" , \"–\" ,\"_\" , \"   \"]\n",
        "numbers = ['۰' , '۱', '۲','۳', '۴', '۵','۶','۷','۸','۹']\n",
        "signs_ir = ['٪' , ',' , '_','–' , '٫' , '\"' ]\n",
        "img_src = ['UFITNPF']\n",
        "end = ['انتهای پیام']\n",
        "# some_unknown_char = ['\\u200c' , '\\u200d' , '\\u200e' , '\\u200f']\n",
        "english = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n",
        "not_used = punctuation + signs + numbers + signs_ir + img_src + english + end \n",
        "print(not_used)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['+', '=', '-', '*', '.', '?', '[', ']', '(', ')', '{', '}', '<', '>', '«', '»', ':', '؟', '؛', '،', '،', '!', '@', '#', '$', '%', '^', '&', '*', '_', '\\\\', '/', '//', '|', '…', '–', '_', '   ', '۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹', '٪', ',', '_', '–', '٫', '\"', 'UFITNPF', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'انتهای پیام']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords_list())\n",
        "print(len(stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqX8EkKEWqVJ",
        "outputId": "050bdacb-f933-4e46-9b98-dbf9ab446a35"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "اینجا در ابتدا میبایستی ما یکسری از حروف‌های بلا استفاده رو حذف کنیم"
      ],
      "metadata": {
        "id": "L_oW7PrLdCeU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCh0qhUXz28z"
      },
      "source": [
        "# Make the change for the not_used word for us\n",
        "for i in range(len(content)):\n",
        "    for l in not_used:\n",
        "        content[i] = content[i].replace(l,\"\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcxQwiX7_nu0"
      },
      "source": [
        "# make copy from the content that preprocess them (it used ^_^)\n",
        "preprocess_content =copy.deepcopy(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5L132YOnyL8"
      },
      "source": [
        "print(content[11])\n",
        "print(content[9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k-3WtctvQLC"
      },
      "source": [
        "# make the stem of the word\n",
        "lstemmer = Stemmer()\n",
        "lstemmer.stem('مظاهری')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ES17OzgDed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ae9dd1-75cd-43a0-e317-5f92922c43b0"
      },
      "source": [
        "# the verb stem\n",
        "lemmatizer = Lemmatizer()\n",
        "verbs = lemmatizer.lemmatize('مظاهری')\n",
        "x = verbs.split('#')\n",
        "print(x)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['مظاهر']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk-Dijh7vmFh"
      },
      "source": [
        "# get tokenize by using the hazm word tokenize\n",
        "normalizer = Normalizer()\n",
        "dictionary_non_posting = collections.defaultdict(list)\n",
        "for i in range(len(content)):\n",
        "  tokenizes = []\n",
        "  tokenizes = word_tokenize(content[i])\n",
        "  tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "  for key , value in tokenizes_count.items():\n",
        "    if key in dictionary_non_posting:\n",
        "      value_old = dictionary_non_posting[key]\n",
        "      dictionary_non_posting[key] = value + value_old\n",
        "    else:\n",
        "      dictionary_non_posting[key] = value "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((dictionary_non_posting['و']))\n",
        "print(len(dictionary_non_posting))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNuBmyKIyZb_",
        "outputId": "6b00d478-72d1-47d2-d9d8-7f2e49fd6eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130255\n",
            "58734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA1p3iTMRx3i"
      },
      "source": [
        "length_dict_non_posting = {k: v for k, v in sorted(dictionary_non_posting.items(), key=lambda item: item[1] , reverse=True)}\n",
        "with open(\"mydict.txt\", 'w') as f: \n",
        "    for key, value in length_dict_non_posting.items(): \n",
        "        f.write(\"%s:%s\\n\" % (key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM8_6dVEAozE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5368012f-ae41-438e-c6be-89ebb1b61ca0"
      },
      "source": [
        "stop_words = ['آنها' , 'پیش' ,'پس' ,'هر' ,'او' ,'یا' ,'نیز' ,'وی' ,'ما' ,'خود' ,'هم' ,'تا' ,'آن' ,'بر' ,'برای' ,'را' ,'با' ,'که' ,'این' ,'از' ,'به' ,'در' ,'و' ]\n",
        "stopwords.update(stop_words)\n",
        "print(len(stopwords))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to find position of each word in contents\n",
        "def position_find(word_to_find , lists):\n",
        "  # words = contents.split()\n",
        "  return  [pos for pos, word in enumerate(lists, start=0) if ((word == word_to_find) )] \n"
      ],
      "metadata": {
        "id": "8OSjWzM-mTnN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MULHAoaIJDXC"
      },
      "source": [
        "# get tokenize by using the hazm word tokenize (the postings list [docid , ferq , [position]])\n",
        "normalizer = Normalizer()\n",
        "dictionary_not_steam = collections.defaultdict(list)\n",
        "for i in range(len(content)):\n",
        "    tokenizes = []\n",
        "    tokenizes = word_tokenize(content[i])\n",
        "    tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "    for key , value in tokenizes_count.items():\n",
        "      if((key not in stopwords)):\n",
        "        index = []\n",
        "        pos_index = position_find(key , tokenizes)\n",
        "        index.append(i)\n",
        "        index.append(value)\n",
        "        index.append(pos_index)\n",
        "        dictionary_not_steam[key].append(index)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary_not_steam['و'])\n",
        "print(len(dictionary_not_steam))\n",
        "print(dictionary_not_steam['یحیی'])\n",
        "print(len(dictionary_not_steam['یحیی']))"
      ],
      "metadata": {
        "id": "HBr6Auv0Asxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om7zuvqY3Ezp"
      },
      "source": [
        "lstemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "def stem_stop(words_content):\n",
        "  for j in range(len(words_content)):\n",
        "        words_content[j] = lstemmer.stem(words_content[j])\n",
        "  return words_content"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmgXZSfL3U1C"
      },
      "source": [
        "# get tokenize by using the hazm word tokenize (the postings list [docid , ferq , [position]])\n",
        "dictionary = collections.defaultdict(list)\n",
        "for i in range(len(content)):\n",
        "    tokenizes = []\n",
        "    tokenizes = word_tokenize(content[i])\n",
        "    tokenizes = stem_stop(tokenizes)\n",
        "    tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "    for key , value in tokenizes_count.items():\n",
        "        if key not in stopwords:\n",
        "          index = []\n",
        "          pos_index = position_find(key , tokenizes)\n",
        "          index.append(i)\n",
        "          index.append(value)\n",
        "          index.append(pos_index)\n",
        "          dictionary[key].append(index)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dictionary))"
      ],
      "metadata": {
        "id": "X-nAg9xFpqCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkg4aSkKO2Xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3514af13-559f-4497-890f-a58449803fcb"
      },
      "source": [
        "# write the postings list in json \n",
        "from google.colab import drive\n",
        "with open(\"posting-list.json\", \"w\") as outfile:\n",
        "    json.dump(dictionary, outfile)\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/IR/'"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read postings list from json\n",
        "with open('posting-list.json') as json_file:\n",
        "    dictionary = json.load(json_file)"
      ],
      "metadata": {
        "id": "TWXSvmz3xmoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dictionary[lstemmer.stem('و')]))\n",
        "print((dictionary[lstemmer.stem('')]))\n",
        "print(dictionary_not_steam[('بین‌الملل')])\n",
        "print(len(dictionary_not_steam[('بین‌الملل')]))\n"
      ],
      "metadata": {
        "id": "5m2V6nLDzVeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-jKzn4ItV0"
      },
      "source": [
        "# the more priority\n",
        "def sort_dict(dictionary):\n",
        "   dicts = dictionary\n",
        "   sorted_dicts = sorted(dicts, key = operator.itemgetter(0, 1) , reverse=False)\n",
        "   all = 0\n",
        "   for i in range(len(sorted_dicts)):\n",
        "     all = all + sorted_dicts[i][1]\n",
        "   return all , sorted_dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter With wrong keyboard language\n",
        "def _multiple_replace(mapping, text):\n",
        "    pattern = \"|\".join(map(re.escape, mapping.keys()))\n",
        "    return re.sub(pattern, lambda m: mapping[m.group()], str(text))\n",
        "\n",
        "def convert_en_characters(input_str):\n",
        "    mapping = {\n",
        "        'q': 'ض',\n",
        "        'w': 'ص',\n",
        "        'e': 'ث',\n",
        "        'r': 'ق',\n",
        "        't': 'ف',\n",
        "        'y': 'غ',\n",
        "        'u': 'ع',\n",
        "        'i': 'ه',\n",
        "        'o': 'خ',\n",
        "        'p': 'ح',\n",
        "        '[': 'ج',\n",
        "        ']': 'چ',\n",
        "        'a': 'ش',\n",
        "        's': 'س',\n",
        "        'd': 'ی',\n",
        "        'f': 'ب',\n",
        "        'g': 'ل',\n",
        "        'h': 'ا',\n",
        "        'j': 'ت',\n",
        "        'k': 'ن',\n",
        "        'l': 'م',\n",
        "        ';': 'ک',\n",
        "        \"'\": 'گ',\n",
        "        'z': 'ظ',\n",
        "        'x': 'ط',\n",
        "        'c': 'ز',\n",
        "        'v': 'ر',\n",
        "        'b': 'ذ',\n",
        "        'n': 'د',\n",
        "        'm': 'پ',\n",
        "        ',': 'و',\n",
        "        '?': '؟',\n",
        "        'C': 'ژ',\n",
        "        'H': 'آ',\n",
        "    }\n",
        "    return _multiple_replace(mapping, input_str)"
      ],
      "metadata": {
        "id": "roKhX9chglXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_en_characters(\"Cdlkhsjd;\")"
      ],
      "metadata": {
        "id": "e-N-xnK0hpHJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0fff5812-5021-4df8-f1ea-d888a7c9989b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ژیمناستیک'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_gYEzMXH_Br"
      },
      "source": [
        "# one word query \n",
        "# this sorted query question by frequency\n",
        "def query_one_word():\n",
        "  query = input(\"enter a word for checking: \")\n",
        "  query = convert_en_characters(query)\n",
        "  time_start = datetime.datetime.now()\n",
        "  normal_query = lstemmer.stem(query)\n",
        "  print(\"Normal word to search {}\".format(normal_query))\n",
        "  # all ,sorted_dict = sort_dict(dictionary[normal_query])\n",
        "  dicts = dictionary[normal_query]\n",
        "  sorted_dict = sorted(dicts, key = operator.itemgetter(1, 2) , reverse=True)\n",
        "  print(sorted_dict)\n",
        "  time_finish = datetime.datetime.now()\n",
        "  print(\"{} results in {} ms\".format(len(sorted_dict), ((time_finish - time_start).total_seconds())*1000))\n",
        "  print(\"id -> title\\n\")\n",
        "  for i in sorted_dict:\n",
        "    print(\"{} -> {}\".format(i[0] , title[i[0]]))\n",
        "  print(sorted_dict)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_one_word()"
      ],
      "metadata": {
        "id": "UTYdTl_UYkkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkxTYZpLFM6z"
      },
      "source": [
        "# the dictionary values is the posting lists\n",
        "# post_list = dictionary[key][i] \n",
        "# the i is iteated \n",
        "# return docID\n",
        "def docID(post_list):\n",
        "        return post_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uANGs5WBFoER"
      },
      "source": [
        "# the dictionary values is the posting lists\n",
        "# post_list = dictionary[key][i] \n",
        "# the i is iteated \n",
        "# return list of position\n",
        "def position(plist):\n",
        "        return plist[2]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xR9FNtGw2y"
      },
      "source": [
        "# position intersection algorithm\n",
        "# get the to word position list and start find k position for p1 , p2\n",
        "def position_intersect(p1,p2,k):\n",
        "        answer = []                                                                     # answer <- ()\n",
        "        len1 = len(p1)\n",
        "        len2 = len(p2)\n",
        "        i = j = 0 \n",
        "        while i != len1 and j != len2:                                                  # while (p1 != nil and p2 != nil)\n",
        "                if docID(p1[i]) == docID(p2[j]):\n",
        "                        l = []                                                          # l <- ()\n",
        "                        pp1 = position(p1[i])                                           # pp1 <- positions(p1)\n",
        "                        pp2 = position(p2[j])                                           # pp2 <- positions(p2)\n",
        "    \n",
        "                        plen1 = len(pp1)\n",
        "                        plen2 = len(pp2)\n",
        "                        ii = jj = 0 \n",
        "                        while ii != plen1:                                              # while (pp1 != nil)\n",
        "                                while jj != plen2:                                      # while (pp2 != nil)\n",
        "                                        if abs(pp1[ii] - pp2[jj]) <= k:                 # if (|pos(pp1) - pos(pp2)| <= k)\n",
        "                                                l.append(pp2[jj])                       # l.add(pos(pp2))\n",
        "                                        elif pp2[jj] > pp1[ii]:                         # else if (pos(pp2) > pos(pp1))\n",
        "                                                break    \n",
        "                                        jj+=1                                           # pp2 <- next(pp2)      \n",
        "                                while l != [] and abs(l[0] - pp1[ii]) > k :             # while (l != () and |l(0) - pos(pp1)| > k)\n",
        "                                        l.remove(l[0])                                  # delete(l[0])\n",
        "                                for ps in l:                                            # for each ps in l\n",
        "                                        answer.append([ docID(p1[i]), pp1[ii], ps ])    # add answer(docID(p1), pos(pp1), ps)\n",
        "                                ii+=1                                                   # pp1 <- next(pp1)\n",
        "                        i+=1                                                            # p1 <- next(p1)\n",
        "                        j+=1                                                            # p2 <- next(p2)\n",
        "                elif docID(p1[i]) < docID(p2[j]):                                       # else if (docID(p1) < docID(p2))\n",
        "                        i+=1                                                            # p1 <- next(p1)                                                        \n",
        "                else:\n",
        "                        j+=1                                                            # p2 <- next(p2)\n",
        "        return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3fcOadkLhV-"
      },
      "source": [
        "# multi word query \n",
        "# this sorted query question by frequency\n",
        "def query_multi():\n",
        "  sorted_dicts = []\n",
        "  query = input(\"enter words for checking: \")\n",
        "  time_start = datetime.datetime.now()\n",
        "  query_split = query.split(\" \")\n",
        "  for q in range(len(query_split)):\n",
        "    query_split[q] = convert_en_characters(query_split[q])\n",
        "  k = 1\n",
        "  if len(query_split) < 2:\n",
        "    print(\"not enough word:(\")\n",
        "    return \n",
        "  for i in range(len(query_split)):\n",
        "    normal_query = lstemmer.stem(query_split[i])\n",
        "    print(\"Normal word to search {}\".format(normal_query))\n",
        "    all , diction =  sort_dict(dictionary[normal_query])\n",
        "    print(all)\n",
        "    print(diction)\n",
        "    sorted_dicts.append((all , diction))\n",
        "  print(sorted_dicts)\n",
        "  sorted_dicts = sorted(sorted_dicts, key = lambda x: x[1])\n",
        "  for x in range(len(sorted_dicts)-1):\n",
        "    if(sorted_dicts[x][0] == 0):\n",
        "      sorted_dicts.pop(x)\n",
        "  print(len(sorted_dicts))\n",
        "  print(\"sorted_dicts: \", sorted_dicts)\n",
        "  if len(sorted_dicts) > 2:\n",
        "    sorted_dicts1 = position_intersect(sorted_dicts[0][1] ,sorted_dicts[1][1] , k)\n",
        "    sorted_dicts1 = sorted(sorted_dicts1, key = lambda x: x[1])\n",
        "    sorted_dicts2 = position_intersect(sorted_dicts[1][1] ,sorted_dicts[2][1] , k)\n",
        "    sorted_dicts2 = sorted(sorted_dicts2, key = lambda x: x[1])\n",
        "    print(\"sorted_dicts1: \", sorted_dicts1)\n",
        "    print(\"sorted_dicts2: \",sorted_dicts2)\n",
        "\n",
        "  elif (len(sorted_dicts) == 2):\n",
        "    sorted_dicts = position_intersect(sorted_dicts[0][1] ,sorted_dicts[1][1] , k)\n",
        "    sorted_dicts = sorted(sorted_dicts, key = lambda x: x[1])\n",
        "    print(sorted_dicts)\n",
        "    res = collections.defaultdict(list)\n",
        "    for x in range(len(sorted_dicts)):\n",
        "      res[sorted_dicts[x][0]].append([sorted_dicts[x][1] , sorted_dicts[x][2]])\n",
        "    res = sorted(res, key=lambda k: len(res[k]), reverse=True)\n",
        "  time_finish = datetime.datetime.now()\n",
        "  print(\"{} results in {} ms\".format(len(res), ((time_finish - time_start).total_seconds())*1000))\n",
        "  print(\"id -> title\\n\")\n",
        "  for i in range(len(res)):\n",
        "    print(\"{} -> {}\".format(res[i] , title[res[i]]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_multi()"
      ],
      "metadata": {
        "id": "a355pung0lwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zipfs_law():\n",
        "  print(sorted(length_dict_non_posting, key=lambda x: x,reverse=True)[:20])\n"
      ],
      "metadata": {
        "id": "rUCrGDr9q-YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipfs_law()"
      ],
      "metadata": {
        "id": "zrTkgnlgsTTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def heap_law():\n",
        "  count = 1000\n",
        "  normalizer = Normalizer()\n",
        "  dictionary_heap = collections.defaultdict(list)\n",
        "  for i in range(count):\n",
        "    tokenizes = []\n",
        "    tokenizes = word_tokenize(content[i])\n",
        "    tokenizes = stem_stop(tokenizes)\n",
        "    tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "    for key , value in tokenizes_count.items():\n",
        "        if key not in stopwords:\n",
        "          index = []\n",
        "          pos_index = position_find(key , tokenizes)\n",
        "          index.append(i)\n",
        "          index.append(value)\n",
        "          index.append(pos_index)\n",
        "          dictionary_heap[key].append(index)\n",
        "  sum = 0\n",
        "  for key , value in dictionary_heap.items():\n",
        "    for x in value:\n",
        "      sum += x[1]\n",
        "  \n",
        "  print(\"sum for tokens for \",count,\" document is: \",sum,\" and length of vocabulary(M) is : \", len(list(dictionary_heap.keys())))"
      ],
      "metadata": {
        "id": "M-twjunisTR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heap_law()"
      ],
      "metadata": {
        "id": "nOVJqoDgu5fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PHASE II"
      ],
      "metadata": {
        "id": "5xb9G8TazrnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase II\n",
        "example = copy.deepcopy(dictionary[lstemmer.stem('یحیی')])\n",
        "N = len(content)\n",
        "for x in example:\n",
        "    n = len(example)\n",
        "    tfidf = (1+math.log((x[1]),10)) * math.log((N/n),10)\n",
        "    x.append(tfidf)\n",
        "print(dictionary[lstemmer.stem('یحیی')])\n",
        "print(example)"
      ],
      "metadata": {
        "id": "pQ2-GPmyu7nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd44fb6-8ede-4653-c28f-f2598eed97f2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, [126]], [47, 2, [160, 163]], [49, 1, [374]], [84, 2, [60, 63]], [128, 1, [50]], [132, 2, [55, 58]], [177, 2, [482, 485]], [223, 1, [6]], [226, 2, [19, 22]], [227, 1, [248]], [231, 1, [168]], [237, 1, [4]], [251, 2, [6, 9]], [268, 2, [4, 7]], [485, 2, [23, 100]], [532, 2, [246, 249]], [601, 1, [50]], [647, 2, [6, 9]], [652, 1, [15]], [653, 1, [183]], [654, 1, [163]], [656, 1, [89]], [659, 1, [104]], [665, 2, [28, 31]], [733, 1, [90]], [736, 1, [6]], [768, 1, [75]], [786, 2, [6, 9]], [791, 2, [83, 86]], [809, 2, [314, 317]], [814, 2, [43, 46]], [816, 1, [139]], [828, 1, [134]], [863, 2, [199, 202]], [864, 2, [78, 81]], [880, 1, [142]], [894, 2, [62, 240]], [928, 1, [14]], [932, 1, [57]], [964, 2, [337, 340]], [986, 1, [261]], [995, 2, [15, 18]], [1013, 2, [16, 83]], [1018, 1, [48]], [1043, 1, [361]], [1087, 1, [11]], [1105, 1, [6]], [1124, 3, [430, 815, 818]], [1133, 2, [16, 19]], [1157, 1, [30]], [1165, 1, [11]], [1177, 1, [35]], [1183, 1, [541]], [1191, 1, [137]], [1211, 1, [121]], [1228, 2, [60, 63]], [1343, 1, [15]], [1345, 1, [90]], [1392, 3, [24, 146, 149]], [1413, 1, [194]], [1948, 1, [1858]], [2073, 2, [45, 332]], [2262, 3, [728, 956, 1039]], [2340, 1, [26]], [2529, 1, [2]], [2571, 2, [2, 165]], [2794, 2, [2, 136]], [3011, 1, [34]], [3253, 2, [346, 354]], [3454, 1, [34]], [3543, 1, [57]], [3601, 1, [35]], [3660, 1, [880]], [3875, 1, [105]], [3887, 1, [39]], [3891, 1, [419]], [4577, 2, [3942, 4189]], [4883, 1, [0]], [5334, 2, [164, 199]], [5646, 1, [2]], [5654, 1, [2]], [6567, 2, [889, 1224]], [6683, 1, [46]], [7206, 2, [17, 130]], [7211, 1, [276]], [7361, 1, [104]], [7415, 2, [5, 304]], [7417, 2, [540, 838]], [7429, 1, [199]], [7466, 1, [133]]]\n",
            "[[1, 1, [126], 1.9243941635871917], [47, 2, [160, 163], 2.503694530307635], [49, 1, [374], 1.9243941635871917], [84, 2, [60, 63], 2.503694530307635], [128, 1, [50], 1.9243941635871917], [132, 2, [55, 58], 2.503694530307635], [177, 2, [482, 485], 2.503694530307635], [223, 1, [6], 1.9243941635871917], [226, 2, [19, 22], 2.503694530307635], [227, 1, [248], 1.9243941635871917], [231, 1, [168], 1.9243941635871917], [237, 1, [4], 1.9243941635871917], [251, 2, [6, 9], 2.503694530307635], [268, 2, [4, 7], 2.503694530307635], [485, 2, [23, 100], 2.503694530307635], [532, 2, [246, 249], 2.503694530307635], [601, 1, [50], 1.9243941635871917], [647, 2, [6, 9], 2.503694530307635], [652, 1, [15], 1.9243941635871917], [653, 1, [183], 1.9243941635871917], [654, 1, [163], 1.9243941635871917], [656, 1, [89], 1.9243941635871917], [659, 1, [104], 1.9243941635871917], [665, 2, [28, 31], 2.503694530307635], [733, 1, [90], 1.9243941635871917], [736, 1, [6], 1.9243941635871917], [768, 1, [75], 1.9243941635871917], [786, 2, [6, 9], 2.503694530307635], [791, 2, [83, 86], 2.503694530307635], [809, 2, [314, 317], 2.503694530307635], [814, 2, [43, 46], 2.503694530307635], [816, 1, [139], 1.9243941635871917], [828, 1, [134], 1.9243941635871917], [863, 2, [199, 202], 2.503694530307635], [864, 2, [78, 81], 2.503694530307635], [880, 1, [142], 1.9243941635871917], [894, 2, [62, 240], 2.503694530307635], [928, 1, [14], 1.9243941635871917], [932, 1, [57], 1.9243941635871917], [964, 2, [337, 340], 2.503694530307635], [986, 1, [261], 1.9243941635871917], [995, 2, [15, 18], 2.503694530307635], [1013, 2, [16, 83], 2.503694530307635], [1018, 1, [48], 1.9243941635871917], [1043, 1, [361], 1.9243941635871917], [1087, 1, [11], 1.9243941635871917], [1105, 1, [6], 1.9243941635871917], [1124, 3, [430, 815, 818], 2.842563521493108], [1133, 2, [16, 19], 2.503694530307635], [1157, 1, [30], 1.9243941635871917], [1165, 1, [11], 1.9243941635871917], [1177, 1, [35], 1.9243941635871917], [1183, 1, [541], 1.9243941635871917], [1191, 1, [137], 1.9243941635871917], [1211, 1, [121], 1.9243941635871917], [1228, 2, [60, 63], 2.503694530307635], [1343, 1, [15], 1.9243941635871917], [1345, 1, [90], 1.9243941635871917], [1392, 3, [24, 146, 149], 2.842563521493108], [1413, 1, [194], 1.9243941635871917], [1948, 1, [1858], 1.9243941635871917], [2073, 2, [45, 332], 2.503694530307635], [2262, 3, [728, 956, 1039], 2.842563521493108], [2340, 1, [26], 1.9243941635871917], [2529, 1, [2], 1.9243941635871917], [2571, 2, [2, 165], 2.503694530307635], [2794, 2, [2, 136], 2.503694530307635], [3011, 1, [34], 1.9243941635871917], [3253, 2, [346, 354], 2.503694530307635], [3454, 1, [34], 1.9243941635871917], [3543, 1, [57], 1.9243941635871917], [3601, 1, [35], 1.9243941635871917], [3660, 1, [880], 1.9243941635871917], [3875, 1, [105], 1.9243941635871917], [3887, 1, [39], 1.9243941635871917], [3891, 1, [419], 1.9243941635871917], [4577, 2, [3942, 4189], 2.503694530307635], [4883, 1, [0], 1.9243941635871917], [5334, 2, [164, 199], 2.503694530307635], [5646, 1, [2], 1.9243941635871917], [5654, 1, [2], 1.9243941635871917], [6567, 2, [889, 1224], 2.503694530307635], [6683, 1, [46], 1.9243941635871917], [7206, 2, [17, 130], 2.503694530307635], [7211, 1, [276], 1.9243941635871917], [7361, 1, [104], 1.9243941635871917], [7415, 2, [5, 304], 2.503694530307635], [7417, 2, [540, 838], 2.503694530307635], [7429, 1, [199], 1.9243941635871917], [7466, 1, [133], 1.9243941635871917]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the tfidf for all of the words\n",
        "def calculate_tfidf(term_dict):\n",
        "  N = len(content)\n",
        "  doc_norm = [0] * N\n",
        "  Champion_list = {}\n",
        "  print(\"The contents size : \" , N)\n",
        "  for key , value in term_dict.items():\n",
        "      value = term_dict[key]\n",
        "      df = len(value)\n",
        "      # print(term_dict[key] , key)\n",
        "      if(df != 0):\n",
        "        # the number of docs that we have value in \n",
        "        idf =  math.log((N/df),10)\n",
        "        for x in value:\n",
        "          # calcualte the tfidf for each docs\n",
        "          tfidf = (1+math.log((x[1]),10)) * idf \n",
        "          doc_norm[x[0]] += tfidf**2 \n",
        "          x.append(tfidf)\n",
        "        value = sorted(value, key=operator.itemgetter(3), reverse=True)\n",
        "        new_value = [idf, value]\n",
        "        term_dict[key] = new_value\n",
        "  return term_dict , doc_norm"
      ],
      "metadata": {
        "id": "dS5Q0WT81KBl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the documents size\n",
        "def Doc_size(term_dict):\n",
        "  N = len(content)\n",
        "  doc_size = [0] * N\n",
        "  for key,value in term_dict.items():\n",
        "      for i in range(len(value)):\n",
        "            doc_size[value[i][0]] = doc_size[value[i][0]] + value[i][1]\n",
        "  return doc_size"
      ],
      "metadata": {
        "id": "bB6-Kiu1a38a"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nomilaze the vectors\n",
        "def Vector_Normalize(term_dict , doc_norm):\n",
        "    print(\"In Vector Normiliziton\")\n",
        "    print(term_dict['یح'])\n",
        "    for key , value in term_dict.items():\n",
        "      for i in range(len(value)):\n",
        "        value[i][3] = value[i][3]/math.sqrt(doc_norm[value[i][0]])\n",
        "    print(term_dict['یح'])\n",
        "    return term_dict"
      ],
      "metadata": {
        "id": "HKZR9XTBBCCT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Token Normalizetion\n",
        "def Normalize_tokens(tokens):\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i] = lstemmer.stem(tokens[i])\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "qF3ttERD67dT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query vector Normalizition\n",
        "def query_vector_normalize(query_dict):\n",
        "  sigma = 0\n",
        "  for key, value in query_dict.items():\n",
        "    sigma = sigma + value**2\n",
        "  for key, value in query_dict.items():\n",
        "    query_dict[key] = value / math.sqrt(sigma)\n",
        "  return query_dict"
      ],
      "metadata": {
        "id": "89UlFamiV__X"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector of the query \n",
        "def vectorize_query(query):\n",
        "    query_token =  word_tokenize(query)\n",
        "    normal_query_token = Normalize_tokens(query_token)\n",
        "    # print(normal_query_token)\n",
        "    term_freq_dict = dict(collections.Counter(normal_query_token))\n",
        "    query_terms = {}\n",
        "    for  term,freq in term_freq_dict.items():\n",
        "        if term in dictionary.keys() and term not in stopwords:\n",
        "            query_terms[term] = int(1+math.log(freq,10))\n",
        "    # query_terms = query_vector_normalize(query_terms)\n",
        "    return query_terms"
      ],
      "metadata": {
        "id": "BtV5Q3LD4x45"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Similiraty of query and docs\n",
        "def CosineScore(query_dict , terms, doc_norm, K):\n",
        "  N = len(content)\n",
        "  scores_dict = {key:0 for key in range(N)}\n",
        "  # scores_dict = [0 for i in range(0, N)]\n",
        "  Final_score = []\n",
        "  founded_doc = 0\n",
        "  for key , value in query_dict.items():\n",
        "    term = terms[key]\n",
        "    idf_term = term[0]\n",
        "    w_q = query_dict[key] * idf_term\n",
        "    postings = term[1]\n",
        "    # print(len(postings))\n",
        "    champion_list = postings[:50]\n",
        "    champion_list = postings\n",
        "    for x in range(len(champion_list)):\n",
        "      w_d =  champion_list[x][3]\n",
        "      scores_dict[champion_list[x][0]] += w_q * w_d\n",
        "      founded_doc += 1\n",
        "  # print({k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1], reverse=True)})\n",
        "\n",
        "  if founded_doc < K:\n",
        "    print(\"Doc it`s not enough in Chmpion lists\")\n",
        "    for key , value in query_dict.items():\n",
        "      if founded_doc < K:\n",
        "        term = terms[key]\n",
        "        idf_term = term[0]\n",
        "        w_q = query_dict[key] * idf_term\n",
        "        postings = term[1]\n",
        "        above_doc = postings[:25]\n",
        "        for x in range(len(above_doc)):\n",
        "          w_d =  above_doc[x][3]\n",
        "          scores_dict[above_doc[x][0]] += w_q * w_d\n",
        "          founded_doc += 1\n",
        "      \n",
        "  for i in range(len(scores_dict)):\n",
        "    if doc_norm[i] == 0 or scores_dict[i] == 0:\n",
        "      continue\n",
        "    scores_dict[i] = scores_dict[i] / math.sqrt(doc_norm[i])\n",
        "    Final_score.append([i , scores_dict[i]])\n",
        "  for i in range(len(Final_score)):\n",
        "    for j in range(i+1, len(Final_score)):\n",
        "      if Final_score[j][1] > Final_score[i][1]:\n",
        "        Final_score[i], Final_score[j] = Final_score[j], Final_score[i]\n",
        "  return Final_score"
      ],
      "metadata": {
        "id": "kY-3A35f9O0k"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results \n",
        "def Show_res(Scores , K, normal_query):\n",
        "  print(\"id -> title\\n\")\n",
        "  print(\"The Five Top Related Docs\\n\")\n",
        "  for i in range(K):\n",
        "    print(\"{} -> {}\".format(Scores[i][0] , title[Scores[i][0]]))\n",
        "  find_secntences(Scores[0][0] , normal_query)\n",
        "  print(title[Scores[0][0]])\n",
        "  print(content[Scores[0][0]])"
      ],
      "metadata": {
        "id": "EY0tsc6-bbin"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_secntences(doc_id , query_token):\n",
        "  sentences = sent_tokenize(df.loc[doc_id].content)\n",
        "  for sen in sentences:\n",
        "    if(query_token[0] in sen):\n",
        "        print(sen)"
      ],
      "metadata": {
        "id": "Kre7PzZiFym1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionarys = copy.deepcopy(dictionary)\n",
        "dictionarys , doc_norm = calculate_tfidf(dictionarys)"
      ],
      "metadata": {
        "id": "eQTpQC-hUnuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d140c04-dfce-466f-c82c-fb5fdf9c49d4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The contents size :  7562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tfidf.pickle', 'wb') as handle:\n",
        "    pickle.dump(dictionarys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('doc_norm.pickle', 'wb') as handle:\n",
        "    pickle.dump(doc_norm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "3c2Y87i0p0Hl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"enter a word for checking: \")\n",
        "time_start = datetime.datetime.now()\n",
        "normal_query = lstemmer.stem(query)\n",
        "print(\"Normal word to search {}\".format(normal_query))\n",
        "query_dict = vectorize_query(query)\n",
        "K = 5\n",
        "Scores = CosineScore(query_dict , dictionarys , doc_norm, K)\n",
        "time_finish = datetime.datetime.now()\n",
        "print(\"{} results in {} ms\".format(len(Scores), ((time_finish - time_start).total_seconds())*1000))\n",
        "Show_res(Scores, K, normal_query)"
      ],
      "metadata": {
        "id": "nNEmy6HPT43K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_one_word()  "
      ],
      "metadata": {
        "id": "vBdKJSiK_FFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_multi()"
      ],
      "metadata": {
        "id": "GmSfJfxX2w8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##This is the Preferential part of Phase II"
      ],
      "metadata": {
        "id": "5QNUj6TbeHJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####my model"
      ],
      "metadata": {
        "id": "ajSGnRCc6YNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "uiatjbmbjrUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "UkD0FNDkefIv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords_list())\n",
        "stop_words = ['آنها' , 'پیش' ,'پس' ,'هر' ,'او' ,'یا' ,'نیز' ,'وی' ,'ما' ,'خود' ,'هم' ,'تا' ,'آن' ,'بر' ,'برای' ,'را' ,'با' ,'که' ,'این' ,'از' ,'به' ,'در' ,'و' ]\n",
        "stopwords.update(stop_words)"
      ],
      "metadata": {
        "id": "nG89779dLtjH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traning_data = []\n",
        "for doc_id in range(len(content)):\n",
        "  word_list = []\n",
        "  word_list = word_tokenize(content[doc_id])\n",
        "  word_list = stem_stop(word_list)\n",
        "  for word in word_list:\n",
        "    if word in stopwords:\n",
        "      word_list.remove(word)\n",
        "  traning_data.append(word_list)"
      ],
      "metadata": {
        "id": "phGEIekkJEMm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(traning_data))"
      ],
      "metadata": {
        "id": "6c9OTWTnK3BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bedd55e-cf47-48b8-f3c6-9fc5bba6f14b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('my_traning_data.pickle', 'wb') as handle:\n",
        "    pickle.dump(traning_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "MtgHbUqtLZw2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handle = open('my_traning_data.pickle', 'rb')\n",
        "traning_data = pickle.load(handle)"
      ],
      "metadata": {
        "id": "wt1rRdtakEvi"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "print(\"# cores \" , cores)"
      ],
      "metadata": {
        "id": "uqnRgpfOjbTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830d0ca4-8755-402c-9684-8d797d8a1c03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# cores  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_num = len(traning_data)\n",
        "tokens_num = sum([len(x) for x in traning_data])\n",
        "print(docs_num)\n",
        "print(tokens_num)"
      ],
      "metadata": {
        "id": "MkXNRAQMkinf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ddfb4e-286a-4205-e2b4-960abff75c5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7562\n",
            "1755713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "w2v_model = Word2Vec(min_count = 1,\n",
        "                     window = 5,\n",
        "                     vector_size = 300,\n",
        "                     alpha = 0.03,\n",
        "                     workers = 2)\n",
        "w2v_model.build_vocab(traning_data)\n",
        "w2v_model_vocab_size = len(w2v_model.wv)\n",
        "print(w2v_model_vocab_size)"
      ],
      "metadata": {
        "id": "I70NEBtulNc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7a736f-ec73-4a0d-9eba-384c342fc3cf"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40373\n",
            "CPU times: user 1.25 s, sys: 4.97 ms, total: 1.26 s\n",
            "Wall time: 1.27 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "w2v_model.train(traning_data , total_examples=w2v_model.corpus_count, epochs= 20)\n",
        "end = time.time()\n",
        "print(\"{} s\".format(end-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDb8eQwvkhjY",
        "outputId": "1eeafe91-8811-497c-f688-21319fddd1c0"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70.4228286743164 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.save('w2v_myModel.model')\n",
        "# w2v_model = Word2Vec.load('w2v_myModel.model')"
      ],
      "metadata": {
        "id": "YhSNLC9-lBfi"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the word vector lookup table\n",
        "len(w2v_model.wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "999PzFajlury",
        "outputId": "c3f70216-f2d7-40c6-810f-e06fcb77e57c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40373"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(w2v_model.wv['یح'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCZvcnAVl3by",
        "outputId": "c2a4b299-6c9c-49b5-963d-21c83ecf60d8"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.45066428e-01,  7.43347704e-02, -5.85016966e-01,  3.98130327e-01,\n",
              "        1.30105770e+00, -5.23056805e-01,  2.07763072e-02, -1.12160794e-01,\n",
              "        2.87920266e-01, -6.32298112e-01,  1.21837461e+00,  1.38908780e+00,\n",
              "       -4.75313753e-01,  2.68529773e-01,  1.00922894e+00, -4.61911410e-01,\n",
              "       -4.11071986e-01,  6.89613700e-01, -7.72028923e-01,  5.76258242e-01,\n",
              "        9.31119084e-01, -1.30945474e-01, -8.72690201e-01, -1.57994461e+00,\n",
              "        5.64066410e-01, -3.06162611e-02, -5.25149345e-01,  2.37967387e-01,\n",
              "       -1.69981495e-01, -8.75502110e-01, -1.48857310e-01, -3.71986777e-01,\n",
              "        6.03224993e-01,  3.45195591e-01,  7.86476552e-01,  5.46946108e-01,\n",
              "       -5.84725678e-01, -1.11062264e+00, -4.76734459e-01,  1.53512910e-01,\n",
              "       -2.24046573e-01, -7.89780021e-01, -1.50433302e-01, -1.28473186e+00,\n",
              "        3.45782071e-01, -5.46696424e-01, -5.58586180e-01, -5.41899949e-02,\n",
              "       -6.95443630e-01, -2.35190347e-01, -9.91130769e-01,  1.16512917e-01,\n",
              "        2.95556575e-01,  5.50308190e-02,  2.57253677e-01,  4.69431430e-01,\n",
              "       -4.58299756e-01,  8.74302834e-02, -1.09600119e-01,  9.67481583e-02,\n",
              "        4.09930676e-01, -1.07865613e-02,  2.37788439e-01, -3.25197101e-01,\n",
              "        5.10434031e-01,  9.50040340e-01, -1.14834122e-01, -4.16050643e-01,\n",
              "        1.60015792e-01, -5.28611481e-01,  7.69577324e-01,  5.16843610e-03,\n",
              "       -1.21327549e-01, -1.56356007e-01, -2.72249132e-01, -4.86617982e-01,\n",
              "       -3.17961574e-02,  8.86929110e-02, -5.72372437e-01,  4.76355888e-02,\n",
              "        2.17381060e-01, -9.79091823e-01, -5.04783571e-01,  5.68389818e-02,\n",
              "        4.78620768e-01,  2.95059919e-01, -5.50171077e-01,  8.86824206e-02,\n",
              "        1.63732958e+00, -2.32886180e-01, -5.58079362e-01, -2.76976168e-01,\n",
              "        5.30729949e-01,  4.82862681e-01, -1.23840463e+00,  2.23314315e-01,\n",
              "       -2.61747509e-01,  4.73563731e-01,  7.46774971e-01,  5.77860594e-01,\n",
              "       -5.97814679e-01,  1.12272084e-01,  3.02297801e-01, -7.77830705e-02,\n",
              "       -3.69475365e-01, -7.81394780e-01, -8.13507140e-01,  4.14879084e-01,\n",
              "       -7.03042984e-01, -1.24370001e-01, -2.42725253e-01, -2.15036273e-01,\n",
              "        2.21888814e-02,  8.02564397e-02,  5.25849342e-01, -1.01534128e+00,\n",
              "       -4.84016240e-01,  1.06700957e-01, -8.81856978e-01, -7.26495922e-01,\n",
              "       -9.16727930e-02, -1.74232423e-01, -5.37014008e-01,  5.56381941e-01,\n",
              "        6.34201944e-01, -2.04470754e+00,  1.64805904e-01, -3.51142168e-01,\n",
              "        6.76759481e-01, -5.40590525e-01, -8.25490713e-01, -8.75909865e-01,\n",
              "        8.22282672e-01, -2.66459823e-01, -4.54084992e-01, -6.68861866e-01,\n",
              "        1.23836434e+00,  4.37558115e-01, -6.49933219e-02, -1.86341572e+00,\n",
              "        3.11423950e-02,  8.62987339e-01, -1.13722610e+00, -5.85431337e-01,\n",
              "       -7.46688426e-01,  5.96904933e-01, -1.36211491e+00,  6.26700640e-01,\n",
              "        7.44763792e-01,  4.47698563e-01, -4.52941149e-01,  4.25898105e-01,\n",
              "        1.93579987e-01, -1.44994330e+00,  9.27160680e-01,  1.74299702e-01,\n",
              "        5.59258997e-01, -2.42583543e-01,  1.46438450e-01,  5.51351964e-01,\n",
              "       -7.45755196e-01,  7.64424682e-01, -2.11561052e-03, -8.47722352e-01,\n",
              "        7.77673364e-01, -2.17074066e-01, -8.35083008e-01, -4.39417869e-01,\n",
              "       -2.27289811e-01,  1.24379170e+00,  7.41959989e-01, -3.49551886e-01,\n",
              "       -3.19228977e-01,  3.62153620e-01,  2.62826663e-02, -1.06413507e+00,\n",
              "       -1.17979944e-02, -2.48145208e-01, -8.55584085e-01,  3.44063282e-01,\n",
              "       -9.02521372e-01, -1.54735714e-01, -1.38038531e-01,  2.50776827e-01,\n",
              "       -1.34129897e-01, -3.12738866e-01, -5.26263714e-01,  1.05408823e+00,\n",
              "       -4.59683150e-01, -3.23848635e-01, -2.86075234e-01,  5.90419888e-01,\n",
              "       -1.19347945e-02,  6.93859160e-01,  2.05428705e-01, -5.11284471e-01,\n",
              "       -2.93144360e-02, -2.55539387e-01, -1.02762446e-01,  7.41877437e-01,\n",
              "       -3.33266079e-01, -5.59987843e-01,  8.91362503e-02,  4.54397798e-01,\n",
              "        3.14612120e-01, -7.79664668e-04, -2.62636598e-02, -5.92862517e-02,\n",
              "       -4.35801834e-01,  1.97214916e-01, -3.85383278e-01, -3.84775817e-01,\n",
              "       -4.84055988e-02, -1.77482978e-01, -3.95876527e-01,  2.34834984e-01,\n",
              "        2.72603601e-01, -9.68095899e-01, -1.54417291e-01, -5.28252125e-01,\n",
              "       -1.63822126e+00, -3.69382918e-01, -6.21066511e-01,  4.97684956e-01,\n",
              "       -2.52746969e-01,  2.45477185e-01,  2.23215725e-02,  8.51400420e-02,\n",
              "        1.09186208e+00, -9.26027477e-01, -5.73685840e-02, -1.30519047e-01,\n",
              "       -1.21077347e+00, -1.40840029e-02, -1.21146142e-01,  3.37787598e-01,\n",
              "        4.22271997e-01, -8.85463774e-01, -3.98556173e-01, -4.73375976e-01,\n",
              "       -5.00467539e-01, -2.06248909e-01,  6.85773551e-01,  1.35623097e-01,\n",
              "       -1.92870304e-01, -9.51803923e-02, -9.46209431e-01,  1.15496688e-01,\n",
              "       -7.26850212e-01, -6.38356268e-01, -1.58870578e+00, -2.98133075e-01,\n",
              "       -8.38956714e-01, -7.88779855e-01,  2.40614861e-01, -1.98754831e-03,\n",
              "        7.60469139e-02,  5.92428267e-01, -1.22921777e+00, -3.39905322e-01,\n",
              "        5.79717040e-01, -6.30865216e-01,  1.14042914e+00, -4.98936832e-01,\n",
              "       -1.99063882e-01, -9.89391327e-01,  6.50530756e-01, -4.76239771e-01,\n",
              "       -3.43708128e-01,  2.35925689e-01,  8.44851434e-01, -2.02029813e-02,\n",
              "        1.11158388e-02,  4.95307446e-01, -3.53537500e-01,  8.97987366e-01,\n",
              "       -4.61043179e-01, -1.91371188e-01,  5.46649098e-01, -2.31372342e-01,\n",
              "        1.25243440e-02, -7.00808704e-01, -5.82356453e-01, -1.62829593e-01,\n",
              "       -6.94750190e-01, -3.23035359e-01, -1.59429610e+00, -9.30002213e-01,\n",
              "       -6.72650933e-01,  3.09031010e-01,  1.24030435e+00,  1.10744201e-01,\n",
              "       -2.20061362e-01,  3.70054185e-01, -2.10665062e-01,  7.55050063e-01,\n",
              "       -6.35233149e-02,  9.10596430e-01,  3.84179801e-01, -1.10948026e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check the similarity"
      ],
      "metadata": {
        "id": "w515rlBqnEx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the same word similar word to this\n",
        "w2v_model.wv.most_similar('بورس')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VRoawOkmxwz",
        "outputId": "ce93fbd1-6cb5-4bf0-a68e-1978d29339fc"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('بهادار', 0.6305003762245178),\n",
              " ('معامل', 0.46025604009628296),\n",
              " ('معاملات', 0.4302796721458435),\n",
              " ('اوراق', 0.42747336626052856),\n",
              " ('فرابورس', 0.41988250613212585),\n",
              " ('بازار', 0.4010249972343445),\n",
              " ('فنارو', 0.38652268052101135),\n",
              " ('\\u200cعال', 0.371860146522522),\n",
              " ('کارگزار', 0.3655050992965698),\n",
              " ('فرو', 0.3618781566619873)]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = w2v_model.wv.similarity('پرسپولیس' ,'استقلال')\n",
        "(score+1)/2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE-jIms6naqQ",
        "outputId": "9086d29e-a288-4931-f241-105f94d80928"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7355090826749802"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Doc_embedding"
      ],
      "metadata": {
        "id": "H8H4flAhqseH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handle = open('tfidf.pickle' , 'rb')\n",
        "docs_tf_idf = pickle.load(handle)"
      ],
      "metadata": {
        "id": "GLQcxaw_qL9Z"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_tf_idf['یح'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icbQ0OxSuBtF",
        "outputId": "2ff672a2-b9e5-4093-a6b5-89b183b19e81"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9243941635871917"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_tfidf = []\n",
        "for i in range(len(content)):\n",
        "  dict_word = {}\n",
        "  for key, value in docs_tf_idf.items():\n",
        "    word = key\n",
        "    for x in value[1]:\n",
        "      if x[0] == i:\n",
        "        dict_word[word] = x[3]\n",
        "  docs_tfidf.append(dict_word)\n"
      ],
      "metadata": {
        "id": "OVKz8E1pr4Zl"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('new_tfidf_doc.pickle', 'wb') as handle:\n",
        "    pickle.dump(docs_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "BQQRx9PtxqP1"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs_tfidf[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61WPiLiTx_eE",
        "outputId": "e314103f-826c-4d57-a13d-683f36f6b1c3"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'مسل': 1.6911159521900536, 'مجدم': 4.480904391835638, 'گفت\\u200cوگو': 0.9569501975429147, 'خبرنگار': 0.6299279374255988, 'ورزش': 0.9210293859664216, 'خبرگزار': 0.33755690524988785, 'فارس': 0.31043494895952184, 'شکس': 1.359384181217871, 'صفر': 1.1519094639999445, 'نف': 1.8452598628640926, 'مسجدسل': 3.2777322171476033, 'مقابل': 1.2037476377190055, 'سپاه': 2.377490339353227, 'اظهار': 0.8141632890985091, 'دا': 0.7043478893589665, 'نمی\\u200cکرد': 2.6376800124829596, 'اتفاق': 1.7423738342862285, 'بیفتد': 1.8533308077617463, 'زود': 1.295437899057894, 'گل': 1.049976776491197, 'خورد': 1.3789495904081128, 'غافلگیر': 3.124443594266751, 'پ': 0.807846720836547, 'مسابق': 0.9411187810091702, 'تن': 0.8356675996333366, 'هفته': 1.192158231137147, 'تمرین': 2.152447805259988, 'شرایط': 1.2637834164717143, 'نمی\\u200cتو': 1.720274180931267, 'ت': 1.0996508381954209, 'بقیه': 1.7817266600184603, 'تیم': 1.3661554470692752, 'مقایسه': 1.7646933207196798, 'برخ': 0.7880259651981101, 'حت': 1.4764429510772679, 'ب': 0.43163377456035434, 'تدارکات': 1.8255582295430968, 'انجا': 0.7038283213702573, 'بدنساز': 1.9595585806504427, 'موقع': 1.3803261192369163, 'تصریح': 0.889186855359825, 'ه': 0.6854488151915289, 'وضعیت': 2.1543608034257278, 'زمین': 1.8422134761603364, 'دس': 0.9440089206880136, 'مشکل': 0.7667023966938351, 'داشته_باش': 1.3971940445242117, 'فوتبال': 1.2708447642989595, 'برد': 1.234198083558678, 'باخ': 1.7483029045315104, 'تلا': 1.1685575352251947, 'می\\u200cکن': 1.2316467923581134, 'امید': 1.1543608034257276, 'خدا': 1.3139706087744272, 'نتیجه': 1.1982316483599298, 'بازی': 1.2543545771908484, 'جبر': 1.5131486881356169, 'کن': 1.1567611743788997, 'ک': 0.8662212982640839, 'رفتن': 1.6637928249788188, 'شخ': 3.276576681698554, 'زدن': 1.6161855832960872, 'ورزشگاه': 1.2701106394493225, 'بهنا': 2.2765766816985544, 'عنو': 0.5112807520004977, '': 0.6930945181721415, 'برا': 0.3268104635300963, 'دید': 1.7627527991170102, 'افتاده_اس': 2.1543608034257278, 'نمی\\u200cدان': 2.400677354105625, 'کار': 0.6651915945582846, 'کس': 1.1875551809035483, 'واقعا': 1.679979586072094, 'ضربه': 1.6533273913006539, 'زد': 1.5094208156163738, 'تهر': 0.8108221618646766, 'اهواز': 2.568355788009266, 'می\\u200cخواست': 2.5169088370089234, 'آماده': 1.5202786701095412, 'شو': 1.3614407750765423, 'نیس': 1.13251240620097, 'بد': 1.4930303994282044, 'جلسه': 0.7534808434459865, 'دروازه': 1.7054504046142427, 'مجبور': 2.2580961176074146, 'برگرد': 3.0214264180647405, 'خانگ': 1.6456405626343626, 'مسابقه': 1.1634693151780588, 'هافبک': 1.9243941635871917, 'اعتراض': 1.562666327569599, 'باشگاه': 1.6981898263116502, 'پرسپولیس': 1.7363138171799353, 'محل': 1.1335618814444592, 'افشین': 2.255387382628616, 'پیروان': 2.5362139922043103, 'اعلا': 0.5821908788201204, 'امتیاز': 1.7688660086988135, 'گف': 0.5201279905704118, 'نمی\\u200cکن': 1.841210175085893, 'بخواهد': 1.9148488456809611, 'بدهد': 1.569006505600618, 'حرف': 1.8977876369251916, 'ز': 0.8344890521477939, 'اس': 0.199656546606942, 'بچه': 1.6044788237628367, 'شراف': 3.69133953323394, 'برو': 1.8104508112803552, 'کجا': 1.8141786837995981, 'دوس': 1.24516821744693, 'میزبان': 1.4755161518506987, 'استفاده': 0.8160546887983536, 'پیک': 2.157795437733525, 'می\\u200cگرفت': 2.7025454139708356, 'قشنگ': 2.674516690370592, 'استعفا': 2.115208679463579, 'مدیرعامل': 1.500238772078379, 'مسئله': 1.3614407750765423, 'مطمئن': 1.8918649387602717, 'دیروز': 1.3830923354800682, 'عصر': 1.3259684569143235, 'سر': 0.9632368378142467, 'خبر': 0.8704624890200904, 'صددرصد': 2.674516690370592, 'هرچه': 1.569006505600618, 'اذ': 2.225424159251173, 'تابع': 2.059092737484648, 'تصم': 1.046766898746015, 'هست': 1.0035754096348166, 'امیدوار': 0.9961121350716362, 'می\\u200cافتد': 1.8657994483213443, 'صلاح': 1.6939452422089178, 'آینده': 0.9543573869646351, 'بحر': 1.5930793640187428, 'زده': 1.8333136942398591, 'پدیده': 2.2226470637198417, 'مشهد': 2.087475077123113, 'قطعا': 1.4088146570483535, 'بتوان': 1.3211294711208585, 'بگیر': 1.485939719766851, 'نگاه': 1.1359115417218184, 'اینطور': 2.2451682174469303, 'می\\u200cخورد': 1.896365439986948, 'بعض': 1.6911159521900536, 'اوق': 2.146242913203548, 'تعویض': 2.1626333293917175, 'مرب': 1.2842441226510901, 'نفع': 1.845212917539567, 'حریف': 1.2283291498945803, 'تما': 0.8616033337277363, 'تلاش': 1.4877015659231376}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_embbedding = []\n",
        "for doc in docs_tfidf:\n",
        "  doc_vec = np.zeros(300)\n",
        "  weights_sum = 0\n",
        "  for token,weight in doc.items():\n",
        "    try:\n",
        "      doc_vec += w2v_model.wv[token] * weight\n",
        "      weights_sum += weight\n",
        "    except Exception:\n",
        "      pass\n",
        "  docs_embbedding.append(doc_vec/weights_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad0TKZJ-qigZ",
        "outputId": "7cd58b9d-2a7e-4eea-a6ef-0dd41694740a"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Docs_similarity"
      ],
      "metadata": {
        "id": "lrBHVzyFw6Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs_embbedding[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmzoOQ8_19z6",
        "outputId": "0aeee0fb-0c1b-4fa8-db51-834ebbb34176"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.18131304  0.05900072  0.10226667 -0.17016551  0.2744504   0.15033492\n",
            " -0.16081591 -0.1103604  -0.19343617 -0.177293    0.2461186  -0.0973968\n",
            "  0.37071955  0.46319689 -0.27752888 -0.21101814 -0.20920031  0.57790799\n",
            " -0.18398261 -0.2341126  -0.11455104 -0.31511761 -0.4721922   0.10455449\n",
            " -0.6479559   0.0835621  -0.1072139  -0.30740328 -0.15245467  0.0936793\n",
            " -0.25603821  0.09869928  0.32098623 -0.07939365 -0.21867852  0.30041877\n",
            " -0.55895036 -0.13572233 -0.6898137   0.11049916  0.00832321  0.1578386\n",
            " -0.12607572 -0.108381    0.03734651  0.05265441 -0.34559598 -0.20046456\n",
            " -0.22665919 -0.12113002 -0.38120905 -0.43400223  0.08972791  0.27563404\n",
            "  0.04629244  0.01057582 -0.46336841 -0.17100977 -0.05262841 -0.17250211\n",
            " -0.05545516 -0.0582168  -0.08360641 -0.00418272 -0.11899657  0.03086686\n",
            "  0.23155678 -0.21816774  0.12817395  0.21450311 -0.03996406  0.13136348\n",
            "  0.31880362  0.31410266 -0.14721934  0.15275533  0.29252495  0.12353271\n",
            "  0.25230951  0.14486119  0.17767684 -0.10232526  0.3479678  -0.13095187\n",
            " -0.19066913 -0.0411806  -0.02547681 -0.06699365 -0.01898766  0.30173164\n",
            " -0.02075049  0.16162627  0.21345287 -0.05283637 -0.2658756   0.16905054\n",
            "  0.17717934  0.07495896  0.25200137 -0.02070927  0.12581472 -0.15379565\n",
            " -0.00855959  0.23033404 -0.37955634  0.10677957 -0.12201844  0.05618353\n",
            " -0.13853334 -0.16796304 -0.28469393  0.01595694 -0.01613064  0.23633388\n",
            "  0.12529606 -0.30274614 -0.37069383 -0.15008365 -0.44825571  0.39485897\n",
            " -0.00468896  0.37911232 -0.19028163 -0.0017647   0.52169067 -0.05121791\n",
            " -0.06561517  0.29163405 -0.17556395  0.49943552  0.11685419 -0.24174954\n",
            "  0.03171484  0.40816448 -0.21238188 -0.055137    0.40390414  0.07712944\n",
            " -0.27737792 -0.36929261  0.33618909 -0.14987467  0.20180308 -0.1450092\n",
            " -0.018255   -0.01117754  0.4235857  -0.23737193 -0.03725054 -0.22393399\n",
            "  0.09914921  0.0189369  -0.22386878 -0.20544683 -0.39188007 -0.29812293\n",
            "  0.30203531  0.28482625  0.16541397 -0.11115551  0.21910009  0.27041405\n",
            "  0.18616692  0.09595347  0.53461421  0.04284124 -0.18515934 -0.22208933\n",
            " -0.62880875 -0.15984156 -0.17320043 -0.02831214 -0.58623316 -0.25662195\n",
            "  0.25391244 -0.44328173 -0.23086468  0.24404378  0.17778966  0.25676371\n",
            "  0.22758158 -0.23456828 -0.26425706  0.41642202 -0.23108515  0.14771074\n",
            "  0.02819503  0.24891422  0.24877188  0.34259788  0.05958413  0.19702224\n",
            "  0.0079007   0.29869136 -0.09980951 -0.18307459  0.41734548 -0.27172353\n",
            " -0.07641296  0.05797706 -0.48002649  0.36493294  0.22469421  0.1874076\n",
            "  0.29917915  0.31963292  0.11862257 -0.26908036 -0.25633183 -0.02674399\n",
            " -0.17338151  0.18911932 -0.01287917  0.32296321 -0.11354192 -0.13515787\n",
            "  0.21944928  0.01861553 -0.14472958 -0.13879175 -0.08859359 -0.08351408\n",
            "  0.1349097  -0.06164096 -0.06994955  0.27458716 -0.23506616 -0.52010306\n",
            " -0.07248188 -0.05108945 -0.02756535 -0.30487502 -0.07186855  0.32126034\n",
            " -0.3146993  -0.11158979 -0.00311421  0.00865509 -0.12655264  0.24053301\n",
            "  0.16056165  0.1256393  -0.00316642  0.15697769 -0.50974225 -0.1739049\n",
            "  0.20650925  0.151743    0.37386337 -0.32268304  0.04120945 -0.04016863\n",
            " -0.18279751 -0.39217999  0.23098183 -0.04074854 -0.19288673 -0.1280942\n",
            " -0.2210595  -0.05320753 -0.42946495 -0.02187743  0.03627097 -0.03043726\n",
            " -0.11739776 -0.06169849  0.21882056 -0.23874443  0.10590568  0.04238866\n",
            " -0.02045139 -0.07015359 -0.311381    0.10640242  0.0343398  -0.02778131\n",
            " -0.09841945  0.67535538  0.3606278  -0.38489866 -0.11147199 -0.60515709\n",
            "  0.45752116  0.08182639 -0.00118134  0.28025027 -0.05586325 -0.24114836\n",
            " -0.0822655  -0.16177484  0.27113497 -0.06231157 -0.34779991 -0.2695993\n",
            " -0.0431461  -0.15404894 -0.19321325  0.20773234  0.03610974  0.00383408]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(doc1, doc2):\n",
        "  similarity_score = np.dot(doc1,doc2)/(norm(doc1) * norm(doc2))\n",
        "  return (similarity_score+1)/2"
      ],
      "metadata": {
        "id": "L3aRf1UAw44X"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity(docs_embbedding[12] ,docs_embbedding[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNbwwJeyzdti",
        "outputId": "3da726ab-a433-43f6-8de7-e2de8f270976"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9275508112729814"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results \n",
        "def Show_res_point(Scores , K):\n",
        "  print(\"id -> title\\n\")\n",
        "  print(\"The Five Top Related Docs\\n\")\n",
        "  for i in range(K):\n",
        "    print(\"{} -> {}\".format(Scores[i][0] , title[Scores[i][0]]))"
      ],
      "metadata": {
        "id": "zah8WL8tG4-1"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"enter a word for checking: \")\n",
        "time_start = datetime.datetime.now()\n",
        "normal_query = lstemmer.stem(query)\n",
        "print(\"Normal word to search {}\".format(normal_query))\n",
        "query_dict = vectorize_query(query)\n",
        "print(query_dict)\n",
        "query_vec = np.zeros(300)\n",
        "weights_sum = 0\n",
        "for token,weight in query_dict.items():\n",
        "  try:\n",
        "    query_vec += w2v_model.wv[token] * weight\n",
        "    weights_sum += weight\n",
        "  except Exception:\n",
        "    pass\n",
        "Scores = []\n",
        "for i in range(len(docs_embbedding)):\n",
        "  Scores.append((i ,similarity( docs_embbedding[i],query_vec)))\n",
        "print(len(Scores))\n",
        "time_finish = datetime.datetime.now()\n",
        "print(\"{} results in {} ms\".format(len(Scores), ((time_finish - time_start).total_seconds())*1000))\n",
        "Scores = sorted(Scores,key = lambda x: x[1], reverse=True)\n",
        "Show_res_point(Scores , K=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu7n7llI1mCh",
        "outputId": "fb5300b4-e9e8-481c-c8aa-3a1a7eca2e6a"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter a word for checking: آیا پرسپولیس از این مرحله لیگ قهرمانان صعود می‌کند ؟\n",
            "Normal word to search آیا پرسپولیس از این مرحله لیگ قهرمانان صعود می‌کند ؟\n",
            "{'پرسپولیس': 1, 'مرحله': 1, 'لیگ': 1, 'قهرمان': 1, 'صعود': 1}\n",
            "7562\n",
            "7562 results in 101.342 ms\n",
            "id -> title\n",
            "\n",
            "The Five Top Related Docs\n",
            "\n",
            "661 -> النصر مدافعش را برای نیمه نهایی لیگ قهرمانان آسیا از دست داد\n",
            "742 -> ژاردیم: فردا شاهد رقابت بهترین تیم‌های آسیا خواهیم بود/به دنبال پیروزی مقابل پرسپولیس هستیم\n",
            "766 -> الهلال با آمار موفقیت آمیز 71 درصدی مقابل پرسپولیس+عکس\n",
            "649 -> واکنش جالب AFC به برد قاطع الهلال مقابل پرسپولیس+عکس\n",
            "669 -> حضور هواداران الهلال  در ورزشگاه الملز مقابل پرسپولیس+فیلم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####your model"
      ],
      "metadata": {
        "id": "qGmqpzx25iqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/gdrive/MyDrive/word2vec/word2vec_model_hazm.zip\" -d \"/content/gdrive/MyDrive/word2vec/word2vec\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFJrF7zT5Fkg",
        "outputId": "2c4f1291-a801-49e4-8f8b-3723c3bb0043"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/word2vec/word2vec_model_hazm.zip\n",
            "  inflating: /content/gdrive/MyDrive/word2vec/word2vec/w2v_150k_hazm_300_v2.model  \n",
            "  inflating: /content/gdrive/MyDrive/word2vec/word2vec/w2v_150k_hazm_300_v2.model.trainables.syn1neg.npy  \n",
            "  inflating: /content/gdrive/MyDrive/word2vec/word2vec/w2v_150k_hazm_300_v2.model.wv.vectors.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model_150K = Word2Vec.load('/content/gdrive/MyDrive/word2vec/word2vec/w2v_150k_hazm_300_v2.model')"
      ],
      "metadata": {
        "id": "-cpotu4TBW4S"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_embbedding_150K = []\n",
        "for doc in docs_tfidf:\n",
        "  doc_vec = np.zeros(300)\n",
        "  weights_sum = 0\n",
        "  for token,weight in doc.items():\n",
        "    try:\n",
        "      doc_vec += w2v_model_150K.wv[token] * weight\n",
        "      weights_sum += weight\n",
        "    except Exception:\n",
        "      pass\n",
        "  docs_embbedding_150K.append(doc_vec/weights_sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7whJZEwB1si",
        "outputId": "6653c9a7-cdef-48dd-e421-115b3eb92c13"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"enter a word for checking: \")\n",
        "time_start = datetime.datetime.now()\n",
        "normal_query = lstemmer.stem(query)\n",
        "print(\"Normal word to search {}\".format(normal_query))\n",
        "query_dict = vectorize_query(query)\n",
        "print(query_dict)\n",
        "query_vec = np.zeros(300)\n",
        "weights_sum = 0\n",
        "for token,weight in query_dict.items():\n",
        "  try:\n",
        "    query_vec += w2v_model_150K.wv[token] * weight\n",
        "    weights_sum += weight\n",
        "  except Exception:\n",
        "    pass\n",
        "Scores = []\n",
        "for i in range(len(docs_embbedding_150K)):\n",
        "  Scores.append((i ,similarity( docs_embbedding_150K[i],query_vec)))\n",
        "print(len(Scores))\n",
        "time_finish = datetime.datetime.now()\n",
        "print(\"{} results in {} ms\".format(len(Scores), ((time_finish - time_start).total_seconds())*1000))\n",
        "Scores = sorted(Scores,key = lambda x: x[1], reverse=True)\n",
        "Show_res_point(Scores , K=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1R8a9fdCKz1",
        "outputId": "f78908de-0bcb-4b1f-b8c7-522653240e6a"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter a word for checking: آیا پرسپولیس از این مرحله لیگ قهرمانان صعود می‌کند ؟\n",
            "Normal word to search آیا پرسپولیس از این مرحله لیگ قهرمانان صعود می‌کند ؟\n",
            "{'پرسپولیس': 1, 'مرحله': 1, 'لیگ': 1, 'قهرمان': 1, 'صعود': 1}\n",
            "7562\n",
            "7562 results in 94.409 ms\n",
            "id -> title\n",
            "\n",
            "The Five Top Related Docs\n",
            "\n",
            "3595 -> پیروزی یاران موسوی با درخشش لژیونر ایران \n",
            "650 -> بهترین بازیکن دیدار پرسپولیس-الهلال مشخص شد\n",
            "3827 -> برزیل مقتدرانه قهرمان لیگ ملت های والیبال شد/ لهستان زانو زد\n",
            "3564 -> خداحافظی تلخ آلکنو با کازان پس از پنجمی زنیت در سوپر لیگ روسیه\n",
            "742 -> ژاردیم: فردا شاهد رقابت بهترین تیم‌های آسیا خواهیم بود/به دنبال پیروزی مقابل پرسپولیس هستیم\n"
          ]
        }
      ]
    }
  ]
}