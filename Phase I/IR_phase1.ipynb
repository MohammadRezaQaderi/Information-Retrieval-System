{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_phase1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OglzFdX3xkWg"
      },
      "source": [
        "pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukukfV3lkc9Y"
      },
      "source": [
        "pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRgP16Yvkdr-"
      },
      "source": [
        "pip install hazm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st5Bw-zmx2Qf"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hazm import *\n",
        "import collections \n",
        "import string\n",
        "import operator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-THkBntGUc7-"
      },
      "source": [
        "import datetime"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22M5ZSWfzzKl"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1jcbbbPTNnQ3NKrPJJL_oc9vrFvHH4UuR'\n",
        "downloaded = drive.CreateFile({'id': file_id})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWQb8M-G0lNd"
      },
      "source": [
        "downloaded.GetContentFile('/IR1_7k_news.xlsx')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc6yuDCtyi3L"
      },
      "source": [
        "# read data\n",
        "df = pd.read_excel(\"/IR1_7k_news.xlsx\")\n",
        "urls = df['url']\n",
        "content = df['content']\n",
        "title = df['title']\n",
        "# normalize data by use hazm\n",
        "normalizer = Normalizer()\n",
        "print(content[11])\n",
        "for i in range(len(content)):\n",
        "  content[i] = normalizer.normalize(content[i])\n",
        "copy_content = content\n",
        "print(content[11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN44N6fh0YXD"
      },
      "source": [
        "punctuation  = [ '+','=', '-' , '*' , '.' ,'?', '[', ']','(',')','{','}','<','>', '«' , '»' ,':' , \"؟\", \"؛\" , \"،\", \"،\" ]\n",
        "signs = [ '!' , '@' , '#' , '$' , '%' , '^' , '&' , '*', '_', '\\\\' , '/' , '//' , '|' , \"…\" , \"–\" ,\"_\"]\n",
        "numbers = ['۰' , '۱', '۲','۳', '۴', '۵','۶','۷','۸','۹']\n",
        "signs_ir = ['٪' , ',' , '_','–' , '٫' , '\"' ]\n",
        "img_src = ['UFITNPF']\n",
        "end = ['انتهای پیام']\n",
        "# some_unknown_char = ['\\u200c' , '\\u200d' , '\\u200e' , '\\u200f']\n",
        "english = list(string.ascii_lowercase) + list(string.ascii_uppercase)\n",
        "not_used = punctuation + signs + numbers + signs_ir + img_src + english + end "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCh0qhUXz28z"
      },
      "source": [
        "for i in range(len(content)):\n",
        "    for l in not_used:\n",
        "        content[i] = content[i].replace(l,\"\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Pb8OQZn6Fg"
      },
      "source": [
        "print(content[11])\n",
        "print(content[9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcxQwiX7_nu0"
      },
      "source": [
        "# make copy from the content that preprocess them (it used ^_^)\n",
        "preprocess_content = content"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5L132YOnyL8"
      },
      "source": [
        "print(content[11])\n",
        "print(content[9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k-3WtctvQLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3898fccf-349c-44f0-8f70-43634d32ecc5"
      },
      "source": [
        "# make the stem of the word\n",
        "lstemmer = Stemmer()\n",
        "lstemmer.stem('اعلام')"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'اعلا'"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ES17OzgDed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b60fdb9-c7f9-4ce6-f7d4-f1160fd72987"
      },
      "source": [
        "# the verb stem\n",
        "lemmatizer = Lemmatizer()\n",
        "verbs = lemmatizer.lemmatize('سازم')\n",
        "x = verbs.split('#')\n",
        "print(x)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ساخت', 'ساز']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk-Dijh7vmFh"
      },
      "source": [
        "# get tokenize by using the hazm word tokenize\n",
        "normalizer = Normalizer()\n",
        "dictionary_non_posting = collections.defaultdict(list)\n",
        "for i in range(len(content)):\n",
        "  tokenizes = []\n",
        "  tokenizes = word_tokenize(content[i])\n",
        "  tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "  for key , value in tokenizes_count.items():\n",
        "    if key in dictionary_non_posting:\n",
        "      value_old = dictionary_non_posting[key]\n",
        "      dictionary_non_posting[key] = value + value_old\n",
        "    else:\n",
        "      dictionary_non_posting[key] = value "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmlpReORTPvN"
      },
      "source": [
        "print((dictionary_non_posting['و']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA1p3iTMRx3i"
      },
      "source": [
        "length_dict_non_posting = {k: v for k, v in sorted(dictionary_non_posting.items(), key=lambda item: item[1] , reverse=True)}\n",
        "with open(\"mydict.txt\", 'w') as f: \n",
        "    for key, value in length_dict_non_posting.items(): \n",
        "        f.write(\"%s:%s\\n\" % (key, value))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM8_6dVEAozE"
      },
      "source": [
        "# stop_words = ['آنها' , 'پیش' ,'پس' ,'هر' ,'او' ,'یا' ,'نیز' ,'وی' ,'ما' ,'خود' ,'هم' ,'تا' ,'آن' ,'بر' ,'برای' ,'را' ,'با' ,'که' ,'این' ,'از' ,'به' ,'در' ,'و' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MULHAoaIJDXC"
      },
      "source": [
        "# get tokenize by using the hazm word tokenize (the postings list [docid , ferq , [position]])\n",
        "normalizer = Normalizer()\n",
        "dictionary_not_steam = collections.defaultdict(list)\n",
        "for i in range(len(content)):\n",
        "    tokenizes = []\n",
        "    tokenizes = word_tokenize(content[i])\n",
        "    tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "    for key , value in tokenizes_count.items():\n",
        "        index = []\n",
        "        pos_index = position_find(key , tokenizes)\n",
        "        index.append(i)\n",
        "        index.append(value)\n",
        "        index.append(pos_index)\n",
        "        dictionary_not_steam[key].append(index)\n",
        "      "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om7zuvqY3Ezp"
      },
      "source": [
        "stopwords = list(stopwords_list())\n",
        "lstemmer = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "def stem_stop(words_content):\n",
        "  for j in range(len(words_content)):\n",
        "    for l in stopwords:\n",
        "      if l == words_content[j]:\n",
        "        words_content[j] = \"\"\n",
        "      else:\n",
        "        words_content[j] = lstemmer.stem(words_content[j])\n",
        "  return words_content"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9zcAdJk6iTU"
      },
      "source": [
        "# to find position of each word in contents\n",
        "def position_find(word_to_find , lists):\n",
        "  # words = contents.split()\n",
        "  return  [pos for pos, word in enumerate(lists, start=0) if word == word_to_find] \n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmgXZSfL3U1C"
      },
      "source": [
        "# get tokenize by using the hazm word tokenize (the postings list [docid , ferq , [position]])\n",
        "normalizer = Normalizer()\n",
        "dictionary = collections.defaultdict(list)\n",
        "for i in range(len(content)):\n",
        "    tokenizes = []\n",
        "    tokenizes = word_tokenize(content[i])\n",
        "    tokenizes = stem_stop(tokenizes)\n",
        "    tokenizes_count = dict(collections.Counter(tokenizes))\n",
        "    for key , value in tokenizes_count.items():\n",
        "        index = []\n",
        "        pos_index = position_find(key , tokenizes)\n",
        "        index.append(i)\n",
        "        index.append(value)\n",
        "        index.append(pos_index)\n",
        "        dictionary[key].append(index)\n",
        "      "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkg4aSkKO2Xt"
      },
      "source": [
        "# with open(\"posting-list.json\", \"w\") as outfile:\n",
        "#     json.dump(dictionary, outfile)\n",
        "with open(\"posting-list.txt\", 'w') as f: \n",
        "    for key, value in dictionary.items(): \n",
        "        f.write('%s:%s\\n' % (key, value))\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlGSPxd8FW0C"
      },
      "source": [
        "\n",
        "print(len(dictionary[lstemmer.stem('صنعتی')]))\n",
        "print((dictionary[lstemmer.stem('صنعتی')]))\n",
        "print(dictionary_not_steam[('بین‌الملل')])\n",
        "print(len(dictionary_not_steam[('بین‌الملل')]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-jKzn4ItV0"
      },
      "source": [
        "# the more priority\n",
        "def sort_dict(dictionary):\n",
        "   dicts = dictionary\n",
        "   sorted_dicts = sorted(dicts, key = operator.itemgetter(0, 1) , reverse=False)\n",
        "   all = 0\n",
        "   \n",
        "   for i in range(len(sorted_dicts)):\n",
        "     all = all + sorted_dicts[i][1]\n",
        "   return all , sorted_dicts"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_gYEzMXH_Br"
      },
      "source": [
        "# one word query \n",
        "# this sorted query question by frequency\n",
        "def query_one_word():\n",
        "  query = input(\"enter a word for checking: \")\n",
        "  time_start = datetime.datetime.now()\n",
        "  normal_query = lstemmer.stem(query)\n",
        "  print(\"Normal word to search {}\".format(normal_query))\n",
        "  # all ,sorted_dict = sort_dict(dictionary[normal_query])\n",
        "  dicts = dictionary[normal_query]\n",
        "  sorted_dict = sorted(dicts, key = operator.itemgetter(1, 2) , reverse=True)\n",
        "  print(sorted_dict)\n",
        "  time_finish = datetime.datetime.now()\n",
        "  print(\"{} results in {} ms\".format(len(sorted_dict), ((time_finish - time_start).total_seconds())*1000))\n",
        "  print(\"id -> title\\n\")\n",
        "  for i in sorted_dict:\n",
        "    print(\"{} -> {}\".format(i[0] , title[i[0]]))\n",
        "  print(sorted_dict)\n",
        "query_one_word()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkxTYZpLFM6z"
      },
      "source": [
        "# the dictionary values is the posting lists\n",
        "# post_list = dictionary[key][i] \n",
        "# the i is iteated \n",
        "# return docID\n",
        "def docID(post_list):\n",
        "        return post_list[0]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uANGs5WBFoER"
      },
      "source": [
        "# the dictionary values is the posting lists\n",
        "# post_list = dictionary[key][i] \n",
        "# the i is iteated \n",
        "# return list of position\n",
        "def position(plist):\n",
        "        return plist[2]    "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xR9FNtGw2y"
      },
      "source": [
        "# position intersection algorithm\n",
        "# get the to word position list and start find k position for p1 , p2\n",
        "def position_intersect(p1,p2,k):\n",
        "        answer = []                                                                     # answer <- ()\n",
        "        len1 = len(p1)\n",
        "        len2 = len(p2)\n",
        "        i = j = 0 \n",
        "        while i != len1 and j != len2:                                                  # while (p1 != nil and p2 != nil)\n",
        "                if docID(p1[i]) == docID(p2[j]):\n",
        "                        l = []                                                          # l <- ()\n",
        "                        pp1 = position(p1[i])                                           # pp1 <- positions(p1)\n",
        "                        pp2 = position(p2[j])                                           # pp2 <- positions(p2)\n",
        "    \n",
        "                        plen1 = len(pp1)\n",
        "                        plen2 = len(pp2)\n",
        "                        ii = jj = 0 \n",
        "                        while ii != plen1:                                              # while (pp1 != nil)\n",
        "                                while jj != plen2:                                      # while (pp2 != nil)\n",
        "                                        if abs(pp1[ii] - pp2[jj]) <= k:                 # if (|pos(pp1) - pos(pp2)| <= k)\n",
        "                                                l.append(pp2[jj])                       # l.add(pos(pp2))\n",
        "                                        elif pp2[jj] > pp1[ii]:                         # else if (pos(pp2) > pos(pp1))\n",
        "                                                break    \n",
        "                                        jj+=1                                           # pp2 <- next(pp2)      \n",
        "                                while l != [] and abs(l[0] - pp1[ii]) > k :             # while (l != () and |l(0) - pos(pp1)| > k)\n",
        "                                        l.remove(l[0])                                  # delete(l[0])\n",
        "                                for ps in l:                                            # for each ps in l\n",
        "                                        answer.append([ docID(p1[i]), pp1[ii], ps ])    # add answer(docID(p1), pos(pp1), ps)\n",
        "                                ii+=1                                                   # pp1 <- next(pp1)\n",
        "                        i+=1                                                            # p1 <- next(p1)\n",
        "                        j+=1                                                            # p2 <- next(p2)\n",
        "                elif docID(p1[i]) < docID(p2[j]):                                       # else if (docID(p1) < docID(p2))\n",
        "                        i+=1                                                            # p1 <- next(p1)                                                        \n",
        "                else:\n",
        "                        j+=1                                                            # p2 <- next(p2)\n",
        "        return answer"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3fcOadkLhV-"
      },
      "source": [
        "# multi word query \n",
        "# this sorted query question by frequency\n",
        "def query_multi():\n",
        "  sorted_dicts = []\n",
        "  query = input(\"enter words for checking: \")\n",
        "  time_start = datetime.datetime.now()\n",
        "  query_split = query.split(\" \")\n",
        "  k = 1\n",
        "  if len(query_split) < 2:\n",
        "    print(\"not enough word:(\")\n",
        "    return \n",
        "  for i in range(len(query_split)):\n",
        "    normal_query = lstemmer.stem(query_split[i])\n",
        "    print(\"Normal word to search {}\".format(normal_query))\n",
        "    all , diction =  sort_dict(dictionary[normal_query])\n",
        "    print(all)\n",
        "    print(diction)\n",
        "    sorted_dicts.append((all , diction))\n",
        "  print(sorted_dicts)\n",
        "  sorted_dicts = sorted(sorted_dicts, key = lambda x: x[1])\n",
        "  \n",
        "  for x in range(len(sorted_dicts)-1):\n",
        "    if(sorted_dicts[x][0] == 0):\n",
        "      sorted_dicts.pop(x)\n",
        "  print(len(sorted_dicts))\n",
        "  if len(sorted_dicts) > 2:\n",
        "    sorted_dicts1 = position_intersect(sorted_dicts[0][1] ,sorted_dicts[1][1] , k)\n",
        "    sorted_dicts1 = sorted(sorted_dicts1, key = lambda x: x[1])\n",
        "    sorted_dicts2 = position_intersect(sorted_dicts[1][1] ,sorted_dicts[2][1] , k)\n",
        "    sorted_dicts2 = sorted(sorted_dicts2, key = lambda x: x[1])\n",
        "    print(\"sorted_dicts1: \", sorted_dicts1)\n",
        "    print(\"sorted_dicts2: \",sorted_dicts2)\n",
        "  #   arr3 = []\n",
        "  #   while(len(sorted_dicts) > 2):\n",
        "  #     arr1 = sorted_dicts[0][1]\n",
        "  #     arr2 = sorted_dicts[1][1]\n",
        "  #     print(sorted_dicts[0])\n",
        "  #     print(sorted_dicts[1])\n",
        "  #     sorted_dicts.pop(0)\n",
        "  #     sorted_dicts.pop(1)\n",
        "  #     arr3 = position_intersect(arr1 ,arr2 , k)\n",
        "  #     sorted_dicts.append((len(arr3), arr3))\n",
        "  #     sorted_dicts = sorted(sorted_dicts, key = lambda x: x[1])\n",
        "    # print(arr3)\n",
        "  elif (len(sorted_dicts) == 2):\n",
        "    sorted_dicts = position_intersect(sorted_dicts[0][1] ,sorted_dicts[1][1] , k)\n",
        "    sorted_dicts = sorted(sorted_dicts, key = lambda x: x[1])\n",
        "    print(sorted_dicts)\n",
        "    res = collections.defaultdict(list)\n",
        "    for x in range(len(sorted_dicts)):\n",
        "      res[sorted_dicts[x][0]].append([sorted_dicts[x][1] , sorted_dicts[x][2]])\n",
        "    print(res)\n",
        "    res = sorted(res, key=lambda k: len(res[k]), reverse=True)\n",
        "    print(res)\n",
        "  time_finish = datetime.datetime.now()\n",
        "  print(\"{} results in {} ms\".format(len(res), ((time_finish - time_start).total_seconds())*1000))\n",
        "  print(\"id -> title\\n\")\n",
        "  for i in range(len(res)):\n",
        "    print(\"{} -> {}\".format(res[i] , title[res[i]]))\n",
        "query_multi()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}